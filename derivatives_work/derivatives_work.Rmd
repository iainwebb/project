---
title: "GPs and differentiation"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
urlcolor: blue
bibliography: references.bib
link-citations: yes
linkcolor: blue
nocite: |
  

header-includes:
  - \usepackage{amsmath}
  - \usepackage{subfig}
  - \usepackage{tcolorbox}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage[font=small,skip=0pt]{caption}
---

Three things are attempted below, each presented in their own section:

\textbf{Section 1:} How to fit a GP to an unknown function.

\textbf{Section 2:} How the GP representation of a derivative of an unknown function represented by a GP is related to the GP representation of the function.

\textbf{Section 3:} How to compare the alignment of two surfaces utilising dot products of their partial derivative vectors.


# Fitting a GP in `R`

The following is taken from @Keirstead_2012, a demo of Gaussian process regression with R by James Keirstead (5 April 2012). He writes:

\begin{tcolorbox}

Chapter 2 of Rasmussen and Williams (\textcolor{blue}{2006}) provides a detailed explanation of the math for Gaussian process regression.  It doesn't provide much in the way of code though.  This Gist is a brief demo of the basic elements of Gaussian process regression, as described on pages 13 to 16.
  
\end{tcolorbox}

```{r, load_packages, include=F}
# Load in the required libraries for data manipulation
# and multivariate normal distribution
require(MASS)
require(plyr)
require(reshape2)
require(ggplot2)
```

## The GP prior

```{r, include=F, cache=T}
x_pts_num <- 50
x_pts_num_s <- 10
```

First, for a (dense) set of `r x_pts_num` regularly-spaced input values (denoted $x^*$) we populate the covariance matrix using the chosen covariance function, or *kernel*, $k(\cdot, \cdot)$, in this case the squared exponential with length parameter $\Psi = 1$. One such covariance matrix $k(x^*, x^*)$ is printed below, but for only `r x_pts_num_s` regularly-spaced points, rather than the `r x_pts_num` used throughout the rest of the work:
<!-- Although the nested loops are ugly, I've checked and it's about 30% faster than a solution using expand.grid() and apply() -->

```{r covariance_matrix_small, include=F, cache=T}
# Parameters:
#	X1, X2 = vectors
# 	l = the scale length parameter
# Returns:
# 	a covariance matrix
calcSigma <- function(X1,X2,l=1) {
  Sigma <- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1))
  for (i in 1:nrow(Sigma)) {
    for (j in 1:ncol(Sigma)) {
      Sigma[i,j] <- exp(-0.5*(abs(X1[i]-X2[j])/l)^2)
    }
  }
  return(Sigma)
}
x.star_s <- seq(0,2*pi, len=x_pts_num_s)
k.xsxs_s <- calcSigma(x.star_s,x.star_s)
```

```{r, echo=F}
# Utility function to print matrices in proper LaTeX format
print_mat <- function(mat) {
  n <- nrow(mat)
  c('\\begin{bmatrix}',
    paste0(sapply(seq_len(n - 1),
                  function(i) paste0(mat[i, ], collapse = ' & ')),
           ' \\\\'),
    paste0(mat[n, ], collapse = ' & '),
    '\\end{bmatrix}')
} 
```

```{r, results = 'asis', echo=F}
writeLines(c('$$',
             print_mat(
               round(k.xsxs_s,3)
             ),
             '$$'))
```

```{r covariance_matrix, include=F, cache=T}
# Parameters:
#	X1, X2 = vectors
# 	l = the scale length parameter
# Returns:
# 	a covariance matrix
calcSigma <- function(X1,X2,l=1) {
  Sigma <- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1))
  for (i in 1:nrow(Sigma)) {
    for (j in 1:ncol(Sigma)) {
      Sigma[i,j] <- exp(-0.5*(abs(X1[i]-X2[j])/l)^2)
    }
  }
  return(Sigma)
}
x.star <- seq(0,2*pi, len=x_pts_num)
k.xsxs <- calcSigma(x.star,x.star)
```

Figure 1 plots some sample functions drawn from the the zero-mean Gaussian process prior with the aforementioned covariance matrix in order to give an idea of the type of functions it specifies.

```{r, include=F, cache=T}
# Generate a number of functions from the process
n.samples_s <- 3
values <- matrix(rep(0,length(x.star)*n.samples_s), ncol=n.samples_s)
for (i in 1:n.samples_s) {
  # Each column represents a sample from a multivariate normal distribution
  # with zero mean and covariance k.xsxs
  values[,i] <- mvrnorm(1, rep(0, length(x.star)), k.xsxs)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")
```

```{r, echo=F, fig.align='center', fig.width=5, fig.height=3, , fig.cap="Three functions sampled from the GP prior distribution.", cache=T}
# Plot the result
fig2a <- ggplot(values,aes(x=x,y=value)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2.25, ymax=2.25, fill="white") +
  geom_line(aes(color=variable), show.legend = FALSE) +
  theme_bw() +
  scale_y_continuous(lim=c(-2,2), name="output, f(x_1, x_2)") +
  xlab("input, x")
fig2a
```

## GP posteriors

```{r, include=F, cache=T}
n <- 5
```

Next, we'll generate `r n` data points using the $\sin(x)$ function in the range $x \in [0.2, 2\pi]$, imagining that that is the underlying real-like function we're interested in learning about. An observation error, randomly sampled from N$(0, 0.1^2)$, is added to each term. The 'observed' values are denoted $z$ and the associated input values $x$. The observations produced are shown in Figure 2, and the observation error added on visible by the fact that they tend not to sit exactly on the function.

```{r real life process function, include=F, cache=T}
real_life_process <- function(x){
  sin(x)
}
```

```{r sample simulation, include=F, cache=T}

# control_inputs <- matrix(seq(1,2*pi-1,len=n), ncol = 1)
control_inputs <- matrix(c(1.0258262, 2.0516523, 3.0774785, 4.2315330, 5.2573591), ncol = 1)

var_meas_err <- 0.01
obs_errors <- matrix(rnorm(n,0,sd = sqrt(var_meas_err)), ncol=1)

zeta <- real_life_process(control_inputs)
obs <- as.matrix(zeta, ncol=1) + obs_errors

f <- data.frame(x=control_inputs,
                y=obs)
colnames(f) <- c("control_inputs", "obs")
```

```{r, echo=F, fig.align='center', fig.width=5, fig.height=3, , fig.cap="The true process, $\\zeta(x)$, is indicated with the black dashed line, and five observations, $z$, by the points.", cache=T}
# Plot the result
fig <- ggplot(f, aes(control_inputs, obs)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2.25, ymax=2.25, fill="white") + 
  geom_point() +
  geom_function(fun = function(x) sin(x), linetype="dashed") +
  xlab("input, x") +
  xlim(0,2*pi) +
  scale_y_continuous(lim=c(-2,2), name="variable of interest") +
  theme_bw()
fig
```

### Noisy-observations but a noise-free GP posterior

To specify the posterior GP distribution, we need three further covariance matrices: $k(x, x)$, $k(x, x^*)$ and $k(x^*, x)$ (recall that $k(\cdot, \cdot)$ was described in Section 1.1). Using these four covariance matrices, the posterior distribution is derived using Equation (2.19) in @rasmussen06:

\begin{align*}
\mathbf{f}^* | x^*, x, \mathbf{f} &\sim N(k(x^*,x) k(x,x)^{-1}\mathbf{y}, \\
& \hspace{1cm} k(x^*, x^*) - k(x^*,x)k(x,x)^{-1}k(x,x^*))
\end{align*}

```{r, include=F, cache=T}
x <- f$control_inputs
k.xx.no_noise <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
# k.xsxs <- calcSigma(x.star,x.star)
```

```{r, include=F, cache=T}
f.star.bar.no_noise <- k.xsx%*%solve(k.xx.no_noise)%*%f$obs
cov.f.star.no_noise <- k.xsxs - k.xsx%*%solve(k.xx.no_noise)%*%k.xxs
```

```{r, include=F, cache=T}
n.samples_m <- 20
```

Next, we'll generate and plot `r n.samples_m` functions from the posterior distribution, along with the mean function and 95% confidence interval (Figure 3). Outside of the interval on which we have data, the mean function returns towards zero.

```{r, include=F, cache=T}
values <- matrix(rep(0,length(x.star)*n.samples_m), ncol=n.samples_m)
for (i in 1:n.samples_m) {
  values[,i] <- mvrnorm(1, f.star.bar.no_noise, cov.f.star.no_noise)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")
```

```{r, include=F, cache=T}
n.samples <- 1000
values_l <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values_l[,i] <- mvrnorm(1, f.star.bar.no_noise, cov.f.star.no_noise)
}
values_l <- cbind(x=x.star,as.data.frame(values_l))
values_l <- melt(values_l,id="x")
```

```{r, include=F, cache=T}
post_summ.no_noise <- data.frame(x.star, f.star.bar.no_noise, rep(NA, length(x.star)), rep(NA, length(x.star)))
colnames(post_summ.no_noise) <- c("x.star", "f.star.bar.no_noise", "lower", "upper")
for (i in 1:length(x.star)) {
  post_summ.no_noise[i, 3] <- quantile(values_l[seq(i, length(x.star)*n.samples, by=length(x.star)),3], 0.025)
}
for (i in 1:length(x.star)) {
  post_summ.no_noise[i, 4] <- quantile(values_l[seq(i, length(x.star)*n.samples, by=length(x.star)),3], 0.975)
}
```

```{r, echo=F, fig.align='center', fig.width=5, fig.height=3, , fig.cap="The posterior mean function (thick black solid line) along with 20 functions sampled from the posterior (grey) and 95% confidence intervals (thin black solid lines). The underlying real-life function is again a dashed black line.", cache=T}
fig2b <- ggplot(values,aes(x=x,y=value)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2.25, ymax=2.25, fill="white") +
  geom_line(aes(group=variable), colour="grey80") + 
  geom_line(data=values_l, aes(x=rep(x.star,n.samples), y=rep(f.star.bar.no_noise,n.samples)),colour="black", linewidth=0.8) +
  geom_point(data=f,aes(x=control_inputs,y=obs)) +
  geom_line(data=post_summ.no_noise,aes(x=x.star,y=lower), colour="black", linewidth=0.25) +
  geom_line(data=post_summ.no_noise,aes(x=x.star,y=upper), colour="black", linewidth=0.25) +
  theme_bw() +
  scale_y_continuous(lim=c(-2,2), name="output, f(x_1, x_2)") +
  xlab("input, x") +
  geom_function(fun = function(x) sin(x), colour="black", linetype="dashed")
fig2b
```

### Noisy-observations and a GP posterior with noise

```{r, include=F, cache=T}
obs_noise <- 0.1
```

Since there is observation error, it might make more sense for the posterior draws *not* to pass through the observed points. This can be achieved by adding a constant (observation noise) term onto the diagonal of the covariance matrix. Recalling that normally distributed measurement errors sampled from N(0, `r var_meas_err`) were added to the simulated values, we'll add `r obs_noise` to the diagonal of the covariance matrix as observation noise.

```{r, include=F, cache=T}
k.xx.noise <- k.xx.no_noise + obs_noise * diag(1, ncol(k.xx.no_noise))
```

```{r, include=F, cache=T}
f.star.bar.noise <- k.xsx%*%solve(k.xx.noise)%*%f$obs
cov.f.star.noise <- k.xsxs - k.xsx%*%solve(k.xx.noise)%*%k.xxs
```

Figure 4 replicates Figure 3 to incorporate this added noise, and now the mean function (thick red line) doesn't pass through the data points, and clearly the posterior uncertainty (thin solid red lines) has increased:

```{r, include=F, cache=T}
values <- matrix(rep(0,length(x.star)*n.samples_m), ncol=n.samples_m)
for (i in 1:n.samples_m) {
  values[,i] <- mvrnorm(1, f.star.bar.noise, cov.f.star.noise)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")
```

```{r, include=F, cache=T}
n.samples <- 1000
values_l <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values_l[,i] <- mvrnorm(1, f.star.bar.noise, cov.f.star.noise)
}
values_l <- cbind(x=x.star,as.data.frame(values_l))
values_l <- melt(values_l,id="x")
```

```{r, include=F, cache=T}
post_summ.noise <- data.frame(x.star, f.star.bar.noise, rep(NA, length(x.star)), rep(NA, length(x.star)))
colnames(post_summ.noise) <- c("x.star", "f.star.bar.noise", "lower", "upper")
for (i in 1:length(x.star)) {
  post_summ.noise[i, 3] <- quantile(values_l[seq(i, length(x.star)*n.samples, by=length(x.star)),3], 0.025)
}
for (i in 1:length(x.star)) {
  post_summ.noise[i, 4] <- quantile(values_l[seq(i, length(x.star)*n.samples, by=length(x.star)),3], 0.975)
}
```

```{r, echo=F, fig.align='center', fig.width=5, fig.height=3, , fig.cap="The posterior mean function (thick black solid line) along with 20 functions sampled from the posterior (grey) and 95% confidence intervals (thin black solid lines). The underlying real-life function is again a dashed black line.", cache=T}
fig2b <- ggplot(values,aes(x=x,y=value)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2.25, ymax=2.25, fill="white") +
  geom_line(aes(group=variable), colour="grey80") + 
  geom_line(data=values_l, aes(x=rep(x.star,n.samples), y=rep(f.star.bar.noise,n.samples)),colour="black", linewidth=0.8) +
  geom_point(data=f,aes(x=control_inputs,y=obs)) +
  geom_line(data=post_summ.noise,aes(x=x.star,y=lower), colour="black", linewidth=0.25) +
  geom_line(data=post_summ.noise,aes(x=x.star,y=upper), colour="black", linewidth=0.25) +
  theme_bw() +
  scale_y_continuous(lim=c(-2,2), name="output, f(x_1, x_2)") +
  xlab("input, x") +
  geom_function(fun = function(x) sin(x), colour="black", linetype="dashed") 
# +
  # geom_errorbar(data=f,aes(x=control_inputs,y=NULL,ymin=obs-2*sqrt(obs_noise), ymax=obs+2*sqrt(obs_noise)), width=0.2)
fig2b
```

<!-- # Recalculate the mean and covariance functions -->
<!-- f.bar.star <- k.xsx%*%solve(k.xx + sigma.n^2*diag(1, ncol(k.xx)))%*%f$y -->
<!-- cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx + sigma.n^2*diag(1, ncol(k.xx)))%*%k.xxs -->

<!-- # Recalulate the sample functions -->
<!-- values <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples) -->
<!-- for (i in 1:n.samples) { -->
<!--   values[,i] <- mvrnorm(1, f.bar.star, cov.f.star) -->
<!-- } -->
<!-- values <- cbind(x=x.star,as.data.frame(values)) -->
<!-- values <- melt(values,id="x") -->

<!-- # Plot the result, including error bars on the observed points -->
<!-- ```{r} -->

<!-- ``` -->

<!-- gg <- ggplot(values, aes(x=x,y=value)) +  -->
<!--   geom_line(aes(group=variable), colour="grey80") + -->
<!--   geom_line(data=NULL,aes(x=rep(x.star,50),y=rep(f.bar.star,50)),colour="red", size=1) +  -->
<!--   geom_errorbar(data=f,aes(x=x,y=NULL,ymin=y-2*sigma.n, ymax=y+2*sigma.n), width=0.2) + -->
<!--   geom_point(data=f,aes(x=x,y=y)) + -->
<!--   theme_bw() + -->
<!--   scale_y_continuous(lim=c(-4,4), name="output, f(x_1, x_2)") + -->
<!--   xlab("input, x") -->
<!-- gg -->

The 95% confidence intervals have clearly changed between Figures 3 and 4. The mean function has slightly changed too -- see Figure 5 -- since Equation 1, which gives a closed-form expression for the mean function, includes (via $\pmb{\alpha}$) involves $k(x,x)$, which is changed when including a nugget versus not including one.

```{r, echo=F, fig.align='center', fig.width=5, fig.height=3, , fig.cap="Using noisy observations for both, we see the posterior mean function from the GP posterior without and with noise (thick black and red lines respectively), along with corresponding 95% confidence intervals (thin black and red lines respectively). These are derived in Sections 1.2.1 and 1.2.2 respectively.", cache=T}
fig2b <- ggplot(values,aes(x=x,y=value)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2.25, ymax=2.25, fill="white") + 
  geom_line(data=values_l, aes(x=rep(x.star,n.samples), y=rep(f.star.bar.no_noise,n.samples)),colour="black", linewidth=0.8) +
  geom_line(data=post_summ.no_noise,aes(x=x.star,y=lower), colour="black", linewidth=0.25) +
  geom_line(data=post_summ.no_noise,aes(x=x.star,y=upper), colour="black", linewidth=0.25) +
  theme_bw() + 
  geom_line(data=values_l, aes(x=rep(x.star,n.samples), y=rep(f.star.bar.noise,n.samples)),colour="red", linewidth=0.8) +
  geom_line(data=post_summ.noise,aes(x=x.star,y=lower), colour="red", linewidth=0.25) +
  geom_line(data=post_summ.noise,aes(x=x.star,y=upper), colour="red", linewidth=0.25) +
  theme_bw() +
  scale_y_continuous(lim=c(-2,2), name="output, f(x_1, x_2)") +
  xlab("input, x")
fig2b
```

\newpage

# The GP representation of a derivative of an unknown function that is represented by a GP

Having fitted a GP to the data, and obtained a posterior mean function (or rather two) that lies close to the real-life underlying function, we turn our attention to the GP representation of a derivative of an unknown function that is represented by a GP. The GP posterior with noise derived in Section 1.2.2 will be used. 

To obtain the derivative of the posterior mean function, we first need an expression for the posterior mean function. Viewing the posterior mean function, $\bar{f}(x^*)$, as a linear combination of `r n` kernel functions, each centered at one of the `r n` training points, Equation (2.27) in @rasmussen06, states that, for any particular input value $x^*$, 

\begin{equation}
\bar{f}(x^*) =  \sum_{i=1}^{5} a_i k(x_i, x^*)
\end{equation}

where $\pmb{\alpha} = (k(x,x) +$ `r obs_noise`$I_5) ^{-1} \mathbf{y}.$ As such, the posterior mean function here will be
\begin{align}
\bar{f}(x^*) &=
\alpha_1 \exp\left(-\frac{1}{2}\left(\frac{x_1-x^*}{l}\right)^2\right) +
\alpha_2 \exp\left(-\frac{1}{2}\left(\frac{x_2-x^*}{l}\right)^2\right) \nonumber \\
&\hspace{0.75cm}
+ \alpha_3 \exp\left(-\frac{1}{2}\left(\frac{x_3-x^*}{l}\right)^2\right) + 
\alpha_4 \exp\left(-\frac{1}{2}\left(\frac{x_4-x^*}{l}\right)^2\right) +
\alpha_5 \exp\left(-\frac{1}{2}\left(\frac{x_5-x^*}{l}\right)^2\right),
\end{align}
which is plotted as the solid black line in Figure 6.

```{r, include=F, cache=T}
alpha <- solve(k.xx.noise) %*% f$obs
```

From @ohagan92, the derivatives of functions modelled by a Gaussian process with
\begin{align*}
E\left\{\eta(x)\right\} &= \mathbf{h}^T(x)\pmb{\beta} \\
Cov\left\{\eta(x), \eta(\mathbf{x'})\right\} &= k(x,x)
\end{align*}
can also be modelled by a Gaussian process, with
\begin{align}
E\left\{\frac{\partial}{\partial x}\eta(x)\right\} &= \frac{\partial}{\partial x}\mathbf{h}^T(x)\pmb{\beta} \\
Cov\left\{\frac{\partial}{\partial x}\eta(x), \frac{\partial}{\partial x}\eta(x')\right\} &= \frac{\partial^2}{\partial x \partial x'}k(x,x'). \nonumber
\end{align}

Led by Equation (2), differentiating the posterior mean function in Equation (1) gives
\begin{align*}
\frac{\partial}{\partial x^*}\bar{f}(x^*) &= \frac{1}{l^2} \left[
\alpha_1 (x_1 - x^*) \exp\left(-\frac{1}{2} \left(\frac{x_1 - x^*}{l}\right)^2\right) +
\dots +
\alpha_5 (x_5 - x^*) \exp\left(-\frac{1}{2} \left(\frac{x_5 - x^*}{l}\right)^2\right)
\right],
\end{align*}

which is also plotted in Figure 6 (solid red line). The real, underlying function $\sin(x)$, and its derivative $\cos(x)$ are included in Figure 6 too (black and red dashed lines respectively) for comparison.

```{r, include=F, cache=T}
alpha1 <- alpha[1]
alpha2 <- alpha[2]
alpha3 <- alpha[3]
alpha4 <- alpha[4]
alpha5 <- alpha[5]
ci1 <- control_inputs[1]
ci2 <- control_inputs[2]
ci3 <- control_inputs[3]
ci4 <- control_inputs[4]
ci5 <- control_inputs[5]
mf <- expression(
  alpha1 * exp(-0.5*((ci1-x)/l)^2) +
    alpha2 * exp(-0.5*((ci2-x)/l)^2) +
    alpha3 * exp(-0.5*((ci3-x)/l)^2) +
    alpha4 * exp(-0.5*((ci4-x)/l)^2) +
    alpha5 * exp(-0.5*((ci5-x)/l)^2))
D(mf, 'x')
```

```{r, echo=F, fig.align='center', fig.width=5, fig.height=3, , fig.cap="The posterior mean function (black solid line) and the true real-life process function, sin($x$) (black  dashed line). Also shown is the derivative of the posterior mean function (red solid line) and the derivative of the true real-life process function, cos($x$) (red dashed line). Observations are shown as black dots.", cache=T}
l <- 1
fig2d <- ggplot(values,aes(x=x,y=value)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2.25, ymax=2.25, fill="white") +
  geom_function(fun = function(x) 
    alpha[1] * exp(-0.5*((control_inputs[1]-x)/l)^2) +
      alpha[2] * exp(-0.5*((control_inputs[2]-x)/l)^2) +
      alpha[3] * exp(-0.5*((control_inputs[3]-x)/l)^2) +
      alpha[4] * exp(-0.5*((control_inputs[4]-x)/l)^2) +
      alpha[5] * exp(-0.5*((control_inputs[5]-x)/l)^2),
    colour="black", linewidth=0.8) +
  geom_function(fun = function(x)
    alpha1 * (exp(-0.5 * ((ci1 - x)/l)^2) * (0.5 * (2 * (1/l * ((ci1 - x)/l))))) + 
      alpha2 * (exp(-0.5 * ((ci2 - x)/l)^2) * (0.5 * (2 * (1/l * ((ci2 - x)/l))))) + 
      alpha3 * (exp(-0.5 * ((ci3 - x)/l)^2) * (0.5 * (2 * (1/l * ((ci3 - x)/l))))) + 
      alpha4 * (exp(-0.5 * ((ci4 - x)/l)^2) * (0.5 * (2 * (1/l * ((ci4 - x)/l))))) + 
      alpha5 * (exp(-0.5 * ((ci5 - x)/l)^2) * (0.5 * (2 * (1/l * ((ci5 - x)/l))))),
    colour="red", linewidth=0.8) +
  geom_function(fun = function(x) sin(x), colour="black", linetype="dashed") +
  geom_function(fun = function(x) cos(x), colour="red", linetype="dashed") +
  geom_point(data=f,aes(x=control_inputs,y=obs)) +
  theme_bw() +
  scale_y_continuous(lim=c(-2,2), name="output, f(x_1, x_2)") +
  xlab("input, x")
fig2d
```

With only five observations, both the posterior mean function of the unknown function and that of the derivative of the unknown function lie close to their true counterparts in the region in which the data lie (and less so outside of this range).

\newpage

# How to compare the alignment of two gradient fields

To define the comparison measure, we'll work through the steps \textcircled{\raisebox{-0.9pt}{\textbf{1}}} - \textcircled{\raisebox{-0.9pt}{\textbf{7}}} outlined in the correspondence between Jonathan and Manolis. 

\textcircled{\raisebox{-0.9pt}{\textbf{1}}} \textbf{Computation of partial derivatives w.r.t. the parameters used the test the case.}

The function against which the others will be compared is:

$$
f(x_1, x_2) = \sqrt{100 - x_1^2 - x_2^2}
$$

which has the following partial derivatives, with slope field and contour plot shown in Figure 7:

$$
\begin{aligned}
\frac{\partial f(x_1, x_2)}{\partial x_1} &=
-\frac{x_1}{\sqrt{100 - x_1^2 - x_2^2}} \\
\frac{\partial (x)}{\partial x_2} &=
-\frac{x_2}{\sqrt{100 - x_1^2 - x_2^2}}
\end{aligned}
$$

```{r, include=F}
Df1_x1 <- function(x1, x2){
  (1/2)*(100 - x1^2 - x2^2)^(-1/2) * (-2*x1)
}
Df1_x2 <- function(x1, x2){
  (1/2)*(100 - x1^2 - x2^2)^(-1/2) * (-2*x2)
}
```

```{r, figures-side, fig.show="hold", out.width="50%", fig.height=6.75, echo=F, fig.cap="Slope field and contour plot for the function $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$", warning=F}
f1 <- expression(
  (100 - x1^2 - x2^2)^(1/2)
)
# D(f1, "x1")
# D(f1, "x2")

vector_field_1 <- expand.grid(x1=seq(0, 5, 1), x2=seq(0, 5, 1))
vector_field_1$vx1 <- with(vector_field_1, Df1_x1(x1, x2))
vector_field_1$vx2 <- with(vector_field_1, Df1_x2(x1, x2))

library(grid)
arrow_scale <- 1
ggplot(vector_field_1, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+vx1/arrow_scale, yend = x2+vx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  xlim(0,5) +
  ylim(0,5)

library(graphics)

# Data
x <- -10:10
y <- -10:10
z <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z, xlim = c(0,5), ylim = c(0,5), nlevels = 50, labcex=1, xlab="x1", ylab="x2")
```

The relative values of the partial derivatives (w.r.t. $x_1$ and $x_2$) at 36 points can be inferred from the slope field in Figure 7.

```{r, include=F, message=F}
library(dplyr)
```

```{r, echo=F}
n_vector_field_1 <- vector_field_1
for (i in 1:36) {
  n_vector_field_1$vx1[i] <- vector_field_1$vx1[i] / sqrt((vector_field_1$vx1[i])^2 + (vector_field_1$vx2[i])^2)
}
for (i in 1:36) {
  n_vector_field_1$vx2[i] <- vector_field_1$vx2[i] / sqrt((vector_field_1$vx1[i])^2 + (vector_field_1$vx2[i])^2)
}
```

```{r, include=F}
n_vector_field_1 <- n_vector_field_1 %>% replace(is.na(.), 0)
```

The remaining six steps require the selection of another function (which we'll call $g(x_1, x_2)$) to compare to $f(x_1, x_2)$ from Step 1. First we'll look at choices for this second function which offer an acceptable match; initially one which is a perfect match.

\newpage

## Good agreement

### Exact match

\textcircled{\raisebox{-0.9pt}{\textbf{2}}} \textbf{Repeat Step 1 for the second response surface.}

Since we're matching $g(x_1, x_2)$ with $f(x_1, x_2)$, we have

$$
g(x_1, x_2) = \sqrt{100 - x_1^2 - x_2^2}
$$

```{r, include=F}
Df2_x1 <- function(x1, x2){
  (1/2)*(100 - x1^2 - x2^2)^(-1/2) * (-2*x1)
}
Df2_x2 <- function(x1, x2){
  (1/2)*(100 - x1^2 - x2^2)^(-1/2) * (-2*x2)
}
```

which has the following partial derivatives, with slope field and contour plot shown in Figure 8:

$$
\begin{aligned}
\frac{\partial g(x_1, x_2)}{\partial x_1} &=
-\frac{x_1}{\sqrt{100 - x_1^2 - x_2^2}} \\
\frac{\partial g(x_1, x_2)}{\partial x_2} &=
-\frac{x_2}{\sqrt{100 - x_1^2 - x_2^2}}
\end{aligned}
$$

```{r, fig.show="hold", out.width="50%", fig.height = 6.75, echo=F, fig.cap="Slope field and contour plot for the function $g(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$", warning=F}
f2 <- expression(
  (100 - x1^2 - x2^2)^(1/2)
)
# D(f1, "x1")
# D(f1, "x2")

vector_field_2 <- expand.grid(x1=seq(0, 5, 1), x2=seq(0, 5, 1))
vector_field_2$vx1 <- with(vector_field_2, Df2_x1(x1, x2))
vector_field_2$vx2 <- with(vector_field_2, Df2_x2(x1, x2))

library(grid)
arrow_scale <- 1
ggplot(vector_field_2, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+vx1/arrow_scale, yend = x2+vx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  xlim(0,5) +
  ylim(0,5)

library(graphics)

# Data
x <- -10:10
y <- -10:10
z <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

\textcircled{\raisebox{-0.9pt}{\textbf{3}}} \textbf{Normalise vectors of partial derivatives.}

This is done, and a slope field showing these normalised vectors is shown in Figure 9.

```{r, include=F}
normal_vector_field_1 <- vector_field_1
colnames(normal_vector_field_1) <- c("x1", "x2", "nvx1", "nvx2")
normal_vector_field_1$nvx1 <- vector_field_1$vx1 / sqrt((vector_field_1$vx1)^2 + (vector_field_1$vx2)^2)
normal_vector_field_1$nvx2 <- vector_field_1$vx2 / sqrt((vector_field_1$vx1)^2 + (vector_field_1$vx2)^2) 
normal_vector_field_1 <- replace(normal_vector_field_1, is.na(normal_vector_field_1), 0)
```

```{r, include=F}
normal_vector_field_2 <- vector_field_2
colnames(normal_vector_field_2) <- c("x1", "x2", "nvx1", "nvx2")
normal_vector_field_2$nvx1 <- vector_field_2$vx1 / sqrt((vector_field_2$vx1)^2 + (vector_field_2$vx2)^2)
normal_vector_field_2$nvx2 <- vector_field_2$vx2 / sqrt((vector_field_2$vx1)^2 + (vector_field_2$vx2)^2) 
normal_vector_field_2 <- replace(normal_vector_field_2, is.na(normal_vector_field_2), 0)
```

```{r, fig.show="hold", out.width="40%", fig.height = 6, echo=F, fig.cap="Normalised slope fields for the functions $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$ and $g(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$.", warning=F}
library(grid)
arrow_scale <- 1
ggplot(normal_vector_field_1, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+nvx1/arrow_scale, yend = x2+nvx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25)

library(grid)
arrow_scale <- 1
ggplot(normal_vector_field_2, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+nvx1/arrow_scale, yend = x2+nvx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25)
```

Steps \textcircled{\raisebox{-0.9pt}{\textbf{4}}}-\textcircled{\raisebox{-0.9pt}{\textbf{6}}} are performed together, and the result of Step \textcircled{\raisebox{-0.9pt}{\textbf{6}}} is discussed in Step \textcircled{\raisebox{-0.9pt}{\textbf{7}}}.

\textcircled{\raisebox{-0.9pt}{\textbf{4}}} \textbf{Compute the dot product and take the absolute values.}

\textcircled{\raisebox{-0.9pt}{\textbf{5}}} \textbf{Repeat the process over the entire parameter space.}

```{r, include=F}
abs(normal_vector_field_1$nvx1 * normal_vector_field_2$nvx1 + normal_vector_field_1$nvx2 * normal_vector_field_2$nvx2)
```

\textcircled{\raisebox{-0.9pt}{\textbf{6}}} \textbf{Sum all together and divide by the number of discrete points.}

\textcircled{\raisebox{-0.9pt}{\textbf{7}}} \textbf{Check how close this number is to 1.}

```{r, include=F}
measure_1 <- sum(abs(normal_vector_field_1$nvx1 * normal_vector_field_2$nvx1 + normal_vector_field_1$nvx2 * normal_vector_field_2$nvx2)) /
  length(vector_field_1$x1)
```

For functions $f(x_1, x_2)$ and $g(x_1, x_2)$, whose contours plots are reprinted in Figure 10 below, we get a number of \textbf{`r round(measure_1,2)`}. This is extremely close to 1, which is want we would expect, since the contours match exactly. The only reason this is not exactly 1 is that at $(0,0)$, the dot product involves a (in fact two) zero vectors.

```{r, fig.show="hold", out.width="50%", fig.height = 6.75, echo=F, fig.cap="Contour plots for the functions $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$ and $g(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$.", warning=F}
# Data
x <- -10:10
y <- -10:10
z_f <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z_f, xlim = c(0,5), ylim = c(0,5), nlevels = 50)

# Data
z_g <- sqrt(100 - outer(x ^ 2, y ^ 2, "+")) + 5 - 5

contour(x, y, z_g, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

\newpage

## Bad agreement

### First try

\textcircled{\raisebox{-0.9pt}{\textbf{2}}} \textbf{Repeat Step 1 for the second response surface.}

$g(x_1, x_2)$ now takes the form:

$$
g(x_1, x_2) = \sqrt{100 - (x_1-5)^2 - x_2^2}
$$

```{r, include=F}
Df2_x1 <- function(x1, x2){
  (1/2)*(100 - (x1-5)^2 - x2^2)^(-1/2) * (-2*(x1-5))
}
Df2_x2 <- function(x1, x2){
  (1/2)*(100 - (x1-5)^2 - x2^2)^(-1/2) * (-2*x2)
}
```

which has the following partial derivatives, with slope field and contour plot shown in Figure 11:

$$
\begin{aligned}
\frac{\partial g(x_1, x_2)}{\partial x_1} &=
-\frac{x_1 - 5}{\sqrt{100 - (x_1-5)^2 - x_2^2}} \\
\frac{\partial g(x_1, x_2)}{\partial x_2} &=
-\frac{x_2}{\sqrt{100 - (x_1-5)^2 - x_2^2}}
\end{aligned}
$$

```{r, fig.show="hold", out.width="50%", fig.height = 6.75, echo=F, fig.cap="Slope field and contour plot for the function $g(x_1, x_2) = \\sqrt{100 - (x_1-5)^2 - x_2^2}$", warning=F}
f2 <- expression(
  (100 - x1^2 - x2^2)^(1/2)
)
# D(f1, "x1")
# D(f1, "x2")

vector_field_2 <- expand.grid(x1=seq(0, 5, 1), x2=seq(0, 5, 1))
vector_field_2$vx1 <- with(vector_field_2, Df2_x1(x1, x2))
vector_field_2$vx2 <- with(vector_field_2, Df2_x2(x1, x2))

library(grid)
arrow_scale <- 1
ggplot(vector_field_2, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+vx1/arrow_scale, yend = x2+vx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  xlim(0,5) +
  ylim(0,5)

library(graphics)

# Data
x <- -10:10
y <- -10:10
z <- sqrt(100 - outer((x-5) ^ 2, y ^ 2, "+"))

contour(x, y, z, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

\textcircled{\raisebox{-0.9pt}{\textbf{3}}} \textbf{Normalise vectors of partial derivatives.}

This is done, and a slope field showing these normalised vectors is shown in Figure 12.

```{r, include=F}
normal_vector_field_1 <- vector_field_1
colnames(normal_vector_field_1) <- c("x1", "x2", "nvx1", "nvx2")
normal_vector_field_1$nvx1 <- vector_field_1$vx1 / sqrt((vector_field_1$vx1)^2 + (vector_field_1$vx2)^2)
normal_vector_field_1$nvx2 <- vector_field_1$vx2 / sqrt((vector_field_1$vx1)^2 + (vector_field_1$vx2)^2) 
normal_vector_field_1 <- replace(normal_vector_field_1, is.na(normal_vector_field_1), 0)
```

```{r, include=F}
normal_vector_field_2 <- vector_field_2
colnames(normal_vector_field_2) <- c("x1", "x2", "nvx1", "nvx2")
normal_vector_field_2$nvx1 <- vector_field_2$vx1 / sqrt((vector_field_2$vx1)^2 + (vector_field_2$vx2)^2)
normal_vector_field_2$nvx2 <- vector_field_2$vx2 / sqrt((vector_field_2$vx1)^2 + (vector_field_2$vx2)^2) 
normal_vector_field_2 <- replace(normal_vector_field_2, is.na(normal_vector_field_2), 0)
```

```{r, fig.show="hold", out.width="40%", fig.height = 6, echo=F, fig.cap="Normalised slope fields for the functions $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$ and $g(x_1, x_2) = \\sqrt{100 - (x_1-5)^2 - x_2^2}$.", warning=F}
library(grid)
arrow_scale <- 1
ggplot(normal_vector_field_1, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+nvx1/arrow_scale, yend = x2+nvx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25)

library(grid)
arrow_scale <- 1
ggplot(normal_vector_field_2, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+nvx1/arrow_scale, yend = x2+nvx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25)
```

Steps \textcircled{\raisebox{-0.9pt}{\textbf{4}}}-\textcircled{\raisebox{-0.9pt}{\textbf{6}}} are performed together, and the result of Step \textcircled{\raisebox{-0.9pt}{\textbf{6}}} is discussed in Step \textcircled{\raisebox{-0.9pt}{\textbf{7}}}.

\textcircled{\raisebox{-0.9pt}{\textbf{4}}} \textbf{Compute the dot product and take the absolute values.}

\textcircled{\raisebox{-0.9pt}{\textbf{5}}} \textbf{Repeat the process over the entire parameter space.}

```{r, include=F}
abs(normal_vector_field_1$nvx1 * normal_vector_field_2$nvx1 + normal_vector_field_1$nvx2 * normal_vector_field_2$nvx2)
```

\textcircled{\raisebox{-0.9pt}{\textbf{6}}} \textbf{Sum all together and divide by the number of discrete points.}

\textcircled{\raisebox{-0.9pt}{\textbf{7}}} \textbf{Check how close this number is to 1.}

```{r, include=F}
measure_2 <- sum(abs(normal_vector_field_1$nvx1 * normal_vector_field_2$nvx1 + normal_vector_field_1$nvx2 * normal_vector_field_2$nvx2)) /
  length(vector_field_1$x1)
```

For functions $f(x_1, x_2)$ and $g(x_1, x_2)$, whose contours plots are reprinted in Figure 13 below, we get a number of \textbf{`r round(measure_2,2)`}.

```{r, fig.show="hold", out.width="50%", fig.height = 6.75, echo=F, fig.cap="Contour plots for the functions $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$ and $g(x_1, x_2) = \\sqrt{100 - (x_1-5)^2 - x_2^2}$.", warning=F}
# Data
x <- -10:10
y <- -10:10
z_f <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z_f, xlim = c(0,5), ylim = c(0,5), nlevels = 50)

# Data
z_g <- sqrt(100 - outer((x-5) ^ 2, y ^ 2, "+")) + 5 - 5

contour(x, y, z_g, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

\newpage

### Second try

\textcircled{\raisebox{-0.9pt}{\textbf{2}}} \textbf{Repeat Step 1 for the second response surface.}

$g(x_1, x_2)$ now takes the form:

$$
g(x_1, x_2) = \sqrt{100 - x_1^2 - (x_2-5)^2}
$$

```{r, include=F}
Df2_x1 <- function(x1, x2){
  (1/2)*(100 - x1^2 - (x2-5)^2)^(-1/2) * (-2*x1)
}
Df2_x2 <- function(x1, x2){
  (1/2)*(100 - x1^2 - (x2-5)^2)^(-1/2) * (-2*(x2-5))
}
```

which has the following partial derivatives, with slope field and contour plot shown in Figure 14:

$$
\begin{aligned}
\frac{\partial g(x_1, x_2)}{\partial x_1} &=
-\frac{x_1}{\sqrt{100 - x_1^2 - (x_2-5)^2}} \\
\frac{\partial g(x_1, x_2)}{\partial x_2} &=
-\frac{x_2-5}{\sqrt{100 - x_1^2 - (x_2-5)^2}}
\end{aligned}
$$

```{r, fig.show="hold", out.width="50%", fig.height = 6.75, echo=F, fig.cap="Slope field and contour plot for the function $g(x_1, x_2) = \\sqrt{100 - x_1^2 - (x_2-5)^2}$", warning=F}
f2 <- expression(
  (100 - x1^2 - x2^2)^(1/2)
)
# D(f1, "x1")
# D(f1, "x2")

vector_field_2 <- expand.grid(x1=seq(0, 5, 1), x2=seq(0, 5, 1))
vector_field_2$vx1 <- with(vector_field_2, Df2_x1(x1, x2))
vector_field_2$vx2 <- with(vector_field_2, Df2_x2(x1, x2))

library(grid)
arrow_scale <- 1
ggplot(vector_field_2, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+vx1/arrow_scale, yend = x2+vx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  xlim(0,5) +
  ylim(0,5)

library(graphics)

# Data
x <- -10:10
y <- -10:10
z <- sqrt(100 - outer((x-5) ^ 2, y ^ 2, "+"))

contour(x, y, z, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

\textcircled{\raisebox{-0.9pt}{\textbf{3}}} \textbf{Normalise vectors of partial derivatives.}

This is done, and a slope field showing these normalised vectors is shown in Figure 15.

```{r, include=F}
normal_vector_field_1 <- vector_field_1
colnames(normal_vector_field_1) <- c("x1", "x2", "nvx1", "nvx2")
normal_vector_field_1$nvx1 <- vector_field_1$vx1 / sqrt((vector_field_1$vx1)^2 + (vector_field_1$vx2)^2)
normal_vector_field_1$nvx2 <- vector_field_1$vx2 / sqrt((vector_field_1$vx1)^2 + (vector_field_1$vx2)^2) 
normal_vector_field_1 <- replace(normal_vector_field_1, is.na(normal_vector_field_1), 0)
```

```{r, include=F}
normal_vector_field_2 <- vector_field_2
colnames(normal_vector_field_2) <- c("x1", "x2", "nvx1", "nvx2")
normal_vector_field_2$nvx1 <- vector_field_2$vx1 / sqrt((vector_field_2$vx1)^2 + (vector_field_2$vx2)^2)
normal_vector_field_2$nvx2 <- vector_field_2$vx2 / sqrt((vector_field_2$vx1)^2 + (vector_field_2$vx2)^2) 
normal_vector_field_2 <- replace(normal_vector_field_2, is.na(normal_vector_field_2), 0)
```

```{r, fig.show="hold", out.width="40%", fig.height = 6, echo=F, fig.cap="Normalised slope fields for the functions $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$ and $g(x_1, x_2) = \\sqrt{100 - x_1^2 - (x_2-5)^2}$.", warning=F}
library(grid)
arrow_scale <- 1
ggplot(normal_vector_field_1, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+nvx1/arrow_scale, yend = x2+nvx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25)

library(grid)
arrow_scale <- 1
ggplot(normal_vector_field_2, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+nvx1/arrow_scale, yend = x2+nvx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25)
```

Steps \textcircled{\raisebox{-0.9pt}{\textbf{4}}}-\textcircled{\raisebox{-0.9pt}{\textbf{6}}} are performed together, and the result of Step \textcircled{\raisebox{-0.9pt}{\textbf{6}}} is discussed in Step \textcircled{\raisebox{-0.9pt}{\textbf{7}}}.

\textcircled{\raisebox{-0.9pt}{\textbf{4}}} \textbf{Compute the dot product and take the absolute values.}

\textcircled{\raisebox{-0.9pt}{\textbf{5}}} \textbf{Repeat the process over the entire parameter space.}

```{r, include=F}
abs(normal_vector_field_1$nvx1 * normal_vector_field_2$nvx1 + normal_vector_field_1$nvx2 * normal_vector_field_2$nvx2)
```

\textcircled{\raisebox{-0.9pt}{\textbf{6}}} \textbf{Sum all together and divide by the number of discrete points.}

\textcircled{\raisebox{-0.9pt}{\textbf{7}}} \textbf{Check how close this number is to 1.}

```{r, include=F}
measure_3 <- sum(abs(normal_vector_field_1$nvx1 * normal_vector_field_2$nvx1 + normal_vector_field_1$nvx2 * normal_vector_field_2$nvx2)) /
  length(vector_field_1$x1)
```

For functions $f(x_1, x_2)$ and $g(x_1, x_2)$, whose contours plots are reprinted in Figure 16 below, we get a number of \textbf{`r round(measure_3,2)`}.

```{r, fig.show="hold", out.width="50%", fig.height = 6.75, echo=F, fig.cap="Contour plots for the functions $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$ and $g(x_1, x_2) = \\sqrt{100 - x_1^2 - (x_2-5)^2}$.", warning=F}
# Data
x <- -10:10
y <- -10:10
z_f <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z_f, xlim = c(0,5), ylim = c(0,5), nlevels = 50)

# Data
z_g <- sqrt(100 - outer(x ^ 2, (y-5) ^ 2, "+")) + 5 - 5

contour(x, y, z_g, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

### Third try

\textcircled{\raisebox{-0.9pt}{\textbf{2}}} \textbf{Repeat Step 1 for the second response surface.}

$g(x_1, x_2)$ now takes the form:

$$
g(x_1, x_2) = \sqrt{100 - (x_1-5)^2 - (x_2-5)^2}
$$

```{r, include=F}
Df2_x1 <- function(x1, x2){
  (1/2)*(100 - (x1-5)^2 - (x2-5)^2)^(-1/2) * (-2*(x1-5))
}
Df2_x2 <- function(x1, x2){
  (1/2)*(100 - (x1-5)^2 - (x2-5)^2)^(-1/2) * (-2*(x2-5))
}
```

which has the following partial derivatives, with slope field and contour plot shown in Figure 17:

$$
\begin{aligned}
\frac{\partial g(x_1, x_2)}{\partial x_1} &=
-\frac{x_1}{\sqrt{100 - (x_1-5)^2 - (x_2-5)^2}} \\
\frac{\partial g(x_1, x_2)}{\partial x_2} &=
-\frac{x_2-5}{\sqrt{100 - (x_1-5)^2 - (x_2-5)^2}}
\end{aligned}
$$

```{r, fig.show="hold", out.width="50%", fig.height = 6.75, echo=F, fig.cap="Slope field and contour plot for the function $g(x_1, x_2) = \\sqrt{100 - (x_1-5)^2 - (x_2-5)^2}$", warning=F}
f2 <- expression(
  (100 - x1^2 - x2^2)^(1/2)
)
# D(f1, "x1")
# D(f1, "x2")

vector_field_2 <- expand.grid(x1=seq(0, 5, 1), x2=seq(0, 5, 1))
vector_field_2$vx1 <- with(vector_field_2, Df2_x1(x1, x2))
vector_field_2$vx2 <- with(vector_field_2, Df2_x2(x1, x2))

library(grid)
arrow_scale <- 1
ggplot(vector_field_2, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+vx1/arrow_scale, yend = x2+vx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  xlim(0,5) +
  ylim(0,5)

library(graphics)

# Data
x <- -10:10
y <- -10:10
z <- sqrt(100 - outer((x-5) ^ 2, y ^ 2, "+"))

contour(x, y, z, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

\textcircled{\raisebox{-0.9pt}{\textbf{3}}} \textbf{Normalise vectors of partial derivatives.}

This is done, and a slope field showing these normalised vectors is shown in Figure 18.

```{r, include=F}
normal_vector_field_1 <- vector_field_1
colnames(normal_vector_field_1) <- c("x1", "x2", "nvx1", "nvx2")
normal_vector_field_1$nvx1 <- vector_field_1$vx1 / sqrt((vector_field_1$vx1)^2 + (vector_field_1$vx2)^2)
normal_vector_field_1$nvx2 <- vector_field_1$vx2 / sqrt((vector_field_1$vx1)^2 + (vector_field_1$vx2)^2) 
normal_vector_field_1 <- replace(normal_vector_field_1, is.na(normal_vector_field_1), 0)
```

```{r, include=F}
normal_vector_field_2 <- vector_field_2
colnames(normal_vector_field_2) <- c("x1", "x2", "nvx1", "nvx2")
normal_vector_field_2$nvx1 <- vector_field_2$vx1 / sqrt((vector_field_2$vx1)^2 + (vector_field_2$vx2)^2)
normal_vector_field_2$nvx2 <- vector_field_2$vx2 / sqrt((vector_field_2$vx1)^2 + (vector_field_2$vx2)^2) 
normal_vector_field_2 <- replace(normal_vector_field_2, is.na(normal_vector_field_2), 0)
```

```{r, fig.show="hold", out.width="40%", fig.height = 6, echo=F, fig.cap="Normalised slope fields for the functions $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$ and $g(x_1, x_2) = \\sqrt{100 - (x_1-5)^2 - (x_2-5)^2}$.", warning=F}
library(grid)
arrow_scale <- 1
ggplot(normal_vector_field_1, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+nvx1/arrow_scale, yend = x2+nvx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25)

library(grid)
arrow_scale <- 1
ggplot(normal_vector_field_2, aes(x = x1, y = x2)) +
  geom_segment(aes(xend = x1+nvx1/arrow_scale, yend = x2+nvx2/arrow_scale),
               arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25)
```

Steps \textcircled{\raisebox{-0.9pt}{\textbf{4}}}-\textcircled{\raisebox{-0.9pt}{\textbf{6}}} are performed together, and the result of Step \textcircled{\raisebox{-0.9pt}{\textbf{6}}} is discussed in Step \textcircled{\raisebox{-0.9pt}{\textbf{7}}}.

\textcircled{\raisebox{-0.9pt}{\textbf{4}}} \textbf{Compute the dot product and take the absolute values.}

\textcircled{\raisebox{-0.9pt}{\textbf{5}}} \textbf{Repeat the process over the entire parameter space.}

```{r, include=F}
abs(normal_vector_field_1$nvx1 * normal_vector_field_2$nvx1 + normal_vector_field_1$nvx2 * normal_vector_field_2$nvx2)
```

\textcircled{\raisebox{-0.9pt}{\textbf{6}}} \textbf{Sum all together and divide by the number of discrete points.}

\textcircled{\raisebox{-0.9pt}{\textbf{7}}} \textbf{Check how close this number is to 1.}

```{r, include=F}
measure_4 <- sum(abs(normal_vector_field_1$nvx1 * normal_vector_field_2$nvx1 + normal_vector_field_1$nvx2 * normal_vector_field_2$nvx2)) /
  length(vector_field_1$x1)
```

For functions $f(x_1, x_2)$ and $g(x_1, x_2)$, whose contours plots are reprinted in Figure 19 below, we get a number of \textbf{`r round(measure_4,2)`}.

```{r, fig.show="hold", out.width="50%", fig.height = 6.75, echo=F, fig.cap="Contour plots for the functions $f(x_1, x_2) = \\sqrt{100 - x_1^2 - x_2^2}$ and $g(x_1, x_2) = \\sqrt{100 - (x_1-5)^2 - (x_2-5)^2}$.", warning=F}
# Data
x <- -10:10
y <- -10:10
z_f <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z_f, xlim = c(0,5), ylim = c(0,5), nlevels = 50)

# Data
z_g <- sqrt(100 - outer((x-5) ^ 2, (y-5) ^ 2, "+")) + 5 - 5

contour(x, y, z_g, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

## Summary

On the next page, side-by-side contour plots for each pair of $f(x_1, x_2)$ and $g(x_1, x_2)$ from Sections 3.1 and 3.2 are presented, along with their value of the alignment measure.

\newpage

```{r, fig.show="hold", out.width="27%", fig.height = 6.9, echo=F, fig.cap=paste("Alignment measure of ", round(measure_1,2),".", sep=""), warning=F, fig.align='center'}
# Data
x <- -10:10
y <- -10:10
z_f <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z_f, xlim = c(0,5), ylim = c(0,5), nlevels = 50)

# Data
z_g <- sqrt(100 - outer(x ^ 2, y ^ 2, "+")) + 5 - 5

contour(x, y, z_g, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

```{r, fig.show="hold", out.width="27%", fig.height = 6.9, echo=F, fig.cap=paste("Alignment measure of ", round(measure_2,2),".", sep=""), warning=F, fig.align='center'}
# Data
x <- -10:10
y <- -10:10
z_f <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z_f, xlim = c(0,5), ylim = c(0,5), nlevels = 50)

# Data
z_g <- sqrt(100 - outer((x-5) ^ 2, y ^ 2, "+")) + 5 - 5

contour(x, y, z_g, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

```{r, fig.show="hold", out.width="27%", fig.height = 6.9, echo=F, fig.cap=paste("Alignment measure of ", round(measure_3,2),".", sep=""), warning=F, fig.align='center'}
# Data
x <- -10:10
y <- -10:10
z_f <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z_f, xlim = c(0,5), ylim = c(0,5), nlevels = 50)

# Data
z_g <- sqrt(100 - outer(x ^ 2, (y-5) ^ 2, "+")) + 5 - 5

contour(x, y, z_g, xlim = c(0,5), ylim = c(0,5), nlevels = 50)
```

```{r, fig.show="hold", out.width="27%", fig.height = 6.9, echo=F, fig.cap=paste("Alignment measure of ", round(measure_4,2),".", sep=""), warning=F, fig.align='center'}
# Data
x <- -10:10
y <- -10:10
z_f <- sqrt(100 - outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z_f, xlim = c(0,5), ylim = c(0,5), nlevels = 50, labcex=1, xlab="x1", ylab="x2")

# Data
z_g <- sqrt(100 - outer((x-5) ^ 2, (y-5) ^ 2, "+")) + 5 - 5

contour(x, y, z_g, xlim = c(0,5), ylim = c(0,5), nlevels = 50, labcex=1, xlab="x1", ylab="x2")
```

\newpage

# Citations